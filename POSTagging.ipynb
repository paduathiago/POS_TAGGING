{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-Speech Tagging - Thiago Pádua - 2020007066\n",
    "A tarefa de Part-of-Speech Tagging (POS) consiste em rotular palavras de um texto de acordo com a sua classe gramatical, como substantivo, verbo, adjetivo. Esse processo é fundamental para diversas aplicações em Processamento de Linguagem Natural (PLN), pois permite uma compreensão mais detalhada da estrutura sintática e semântica das sentenças.\n",
    "\n",
    "Por exemplo, considere a seguinte frase:\n",
    "\"A raposa azul dorme tranquilamente.\"\n",
    "\n",
    "    \"A\" pode receber a tag de artigo definido.\n",
    "    \"raposa\" pode receber a tag de substantivo.\n",
    "    \"azul\" pode receber a tag de adjetivo.\n",
    "    \"dorme\" pode receber a tag de verbo.\n",
    "    \"tranquilamente\" pode receber a tag de advérbio.\n",
    "\n",
    "Neste trabalho, o objetivo é explorar a tarefa de POS Tagging para a língua portuguesa, utilizando o corpus Mac-Morpho. Além disso, será implementado um modelo capaz de classificar palavras com precisão, analisando como o contexto influencia a atribuição de classes gramaticais. Por fim, investigaremos os desafios e as limitações do processo, discutindo as classes que apresentam maior e menor precisão ao longo do experimento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregamento dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_path = \"macmorpho-v3/macmorpho-test.txt\"\n",
    "train_file_path = \"macmorpho-v3/macmorpho-train.txt\"\n",
    "dev_file_path = \"macmorpho-v3/macmorpho-dev.txt\"\n",
    "\n",
    "def load_text_data(file_path):\n",
    "    \"\"\"\n",
    "    :param file_path: path to the .txt file containing the data\n",
    "    :return: List of sentences, where each sentence is a list of tuples (word, tag)\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line:  # Ignore empty lines\n",
    "                # Generate a list of tuples (word, tag)\n",
    "                word_tags = [tuple(word.split('_')) for word in line.split()]\n",
    "                sentences.append(word_tags)\n",
    "    return sentences\n",
    "\n",
    "test_data = load_text_data(test_file_path)\n",
    "train_data = load_text_data(train_file_path)\n",
    "dev_data = load_text_data(dev_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: [('Salto', 'N'), ('sete', 'ADJ')]\n",
      "Sentence 2: [('O', 'ART'), ('grande', 'ADJ'), ('assunto', 'N'), ('da', 'PREP+ART'), ('semana', 'N'), ('em', 'PREP'), ('Nova', 'NPROP'), ('York', 'NPROP'), ('é', 'V'), ('a', 'ART'), ('edição', 'N'), ('da', 'PREP+ART'), ('revista', 'N'), ('\"', 'PU'), ('New', 'NPROP'), ('Yorker', 'NPROP'), ('\"', 'PU'), ('que', 'PRO-KS'), ('está', 'V'), ('nas', 'PREP+ART'), ('bancas', 'N'), ('.', 'PU')]\n",
      "Sentence 3: [('Número', 'N'), ('duplo', 'ADJ'), ('especial', 'ADJ'), (',', 'PU'), ('é', 'V'), ('inteirinho', 'ADJ'), ('dedicado', 'PCP'), ('a', 'PREP'), ('ensaios', 'N'), ('sobre', 'PREP'), ('moda', 'N'), ('.', 'PU')]\n"
     ]
    }
   ],
   "source": [
    "for i, sentence in enumerate(test_data[:3]):\n",
    "    print(f\"Sentence {i+1}: {sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metodologia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lisaterumi/postagger-portuguese\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"lisaterumi/postagger-portuguese\")\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Supress output\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste = \"A cidade mais linda de Minas Gerais é Belo Horizonte\".split(sep=' ')\n",
    "inputs = tokenizer(teste, return_tensors=\"pt\", is_split_into_words=True, padding=True, truncation=True)\n",
    "\n",
    "# Obter os índices das palavras originais correspondentes a cada token\n",
    "word_ids = inputs.word_ids(batch_index=0)  # Adicione o batch_index para evitar erros de dimensão\n",
    "\n",
    "# Move the inputs to the GPU\n",
    "if device.type == \"cuda\":\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: [CLS], Palavra Original: None\n",
      "Token: A, Palavra Original: A\n",
      "Token: cidade, Palavra Original: cidade\n",
      "Token: mais, Palavra Original: mais\n",
      "Token: lin, Palavra Original: linda\n",
      "Token: ##da, Palavra Original: linda\n",
      "Token: de, Palavra Original: de\n",
      "Token: Minas, Palavra Original: Minas\n",
      "Token: Gerais, Palavra Original: Gerais\n",
      "Token: é, Palavra Original: é\n",
      "Token: Belo, Palavra Original: Belo\n",
      "Token: Horizonte, Palavra Original: Horizonte\n",
      "Token: [SEP], Palavra Original: None\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "\n",
    "# Mostrar os tokens e as palavras originais\n",
    "for token, word_id in zip(tokens, word_ids):\n",
    "    palavra_original = teste[word_id] if word_id is not None else None\n",
    "    print(f\"Token: {token}, Palavra Original: {palavra_original}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags preditas para palavras: [('A', 'ART'), ('cidade', 'N'), ('mais', 'ADV'), ('linda', 'ADJ'), ('de', 'PREP'), ('Minas', 'NPROP'), ('Gerais', 'NPROP'), ('é', 'V'), ('Belo', 'NPROP'), ('Horizonte', 'NPROP')]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# Mapear previsões para palavras originais\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "\n",
    "tags = []\n",
    "current_word = None\n",
    "for token, word_id, pred in zip(tokens, word_ids, predictions[0].tolist()):\n",
    "    if word_id is None:  # Ignore special tokens\n",
    "        continue\n",
    "\n",
    "    # Associate tag only with the first token of each word. Repeated ids will be ignored\n",
    "    if word_id != current_word:\n",
    "        tags.append(model.config.id2label[pred])\n",
    "        current_word = word_id\n",
    "\n",
    "print(\"Tags preditas para palavras:\", list(zip(teste, tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def POS_tagging(split_document, tokenizer, model):\n",
    "    \"\"\"\n",
    "    :param split_document: List of words\n",
    "    :param tokenizer: Tokenizer object\n",
    "    :param model: Model object\n",
    "\n",
    "    :return: List of tuples (word, tag)\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    inputs = tokenizer(split_document, return_tensors=\"pt\", is_split_into_words=True, padding=True, truncation=True)\n",
    "\n",
    "    # Get the indices of the original words corresponding to each token\n",
    "    word_ids = inputs.word_ids(batch_index=0)  # batch_index = 0 to avoid dimension errors\n",
    "\n",
    "    # Move the inputs to the GPU\n",
    "    if device.type == \"cuda\":\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "\n",
    "    tags = []\n",
    "    current_word = None\n",
    "    for token, word_id, pred in zip(tokens, word_ids, predictions[0].tolist()):\n",
    "        if word_id is None:  # Ignore special tokens\n",
    "            continue\n",
    "\n",
    "        # Associate tag only with the first token of each word. Repeated ids will be ignored\n",
    "        if word_id != current_word:\n",
    "            tags.append(model.config.id2label[pred])\n",
    "            current_word = word_id\n",
    "\n",
    "    return list(zip(split_document, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unified_test = list(itertools.chain(*test_data))\n",
    "# unified_test_words, unified_test_tags = zip(*unified_test)\n",
    "\n",
    "# print(unified_test_words[:10])\n",
    "# print(unified_test_tags[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_tagging(test_data, tokenizer, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
